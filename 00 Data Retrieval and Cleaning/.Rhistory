if(is.unsorted(x)) {
## do sort `along x'
i <- order(x)
x <- x[i]
y <- y[i]
}
xx <- seq(x[1], x[n], length = nsm) ## << for plotting
## Data and a first smoother:
pm <- par("mar")
op <- par(mar = c(pm[1], pm[2]+0.5,
if("main" %in% names(list(...))) 2.5 else 0.5,
pm[4]),
mgp = c(2, 3/4, 0))
on.exit(par(op)) # reset graphics state on exit
plot(x,y, ...)
if(true.f) {
lines(xx, f(xx), lwd=lwd.tr)
if(p.points)
points(x, f(x), cex= 0.4, col="purple")
}
if(sm.spline)
lines(predict(smooth.spline(x,y), x=xx), col="darkblue")
lines(xx, theta(x,y, xx), col=col.sm, lwd=lwd.sm)
if(is.numeric(x.set) && length(x.set) > 0) {
if(x.bxp) {
## Test rTheta() etc :
rr <- t(replicate(B.bxp, rTheta(x,y, x.set)))
brr <- boxplot(rr, plot = FALSE)
## add these to the graphic
bxp(brr, at = x.set, add = TRUE, axes = FALSE,
boxwex = 0.4*mean(diff(x.set)), pch = 4, cex = 0.6)
} else { # just simple vertical lines going to m^(x.set):
y0 <- par("usr")[3]
segments(x0=x.set, y0=y0, y1=theta(x,y, x.set), lty=2, col="gray30")
}
}
}
if (FALSE) # not in 'BATCH'
plot(f, -2,2, col=2, lwd=2, main = body(f))
n <- 100
##------
## To give the exact same data as in 2002-2004 version of Skript :
## 1) "PB":
if(FALSE)
RNGversion("1.6.2")
set.seed(12)
## "New":
## 2) set.seed(144)
set.seed(0)
## 3)
set.seed(1)# <-- finally ``works'' (x = -1 is "peculiar" -> 0.823)
require(sfsmisc) # for pdf.latex() -- pdf.end(), mult.fig() ..
## Generate data   (using set.seed() from above  (or *random* when repeated)
x <- rnorm(n)
y <- f(x) + rnorm(n)
if(!interactive()) { ## data + fit ---> Fig. 5.3 in lecture notes (p.48)
pdf.latex(file="doubleboot-data.pdf", version="1.7", width=7, height=6)
plotData(x,y, x.set=-1:1, x.bxp=FALSE, true.f=FALSE)
dev.off() # pdf.end()
}
## data + true f() + fit
plotData(x,y, x.set=NULL, sm.spline=FALSE)
## x.set <- c(-1,0,1) ## use more interesting one: c(-0.5, 0, 1)
## larger x.set -- is not much slower:
str(x.set <- seq(-1.5, 1.5, by = 1/8)) # [1:25]
## Now add bootstrap m^*(x_0) for these x0
if(!interactive()) ## but nice for demo
plotData(x,y, x.set, sm.spline=FALSE)
### Nominal levels we want to look at: (2017: start from 0.76)
alphas <- 1 - c(seq(0.76, 0.90, by = 0.02),
seq(0.91, 0.95, by = 0.01),
seq(0.955,0.995,by = 0.005), 0.999, 0.9999)
## 1 - a = 0.70 ... 0.9999
(n.a <- length(alphas)) # 24
(th <- theta(x,y, x.set)) # the 'real' \hat{theta}  {for x = x.set}
M <-  500
B <- 1000
## simple bootstrap (only used at the end, for comparison):
system.time( th.B <- replicate(B, rTheta(x,y, x.set)) ) # 0.2 sec [2011]
dim(th.B)##  |n_x|  x  B  = 25 x 1000
boxplot(t(th.B)) # a box plot for each x=x.set[j] with m^*(x) values
plotData(x,y, x.set, sm.spline=FALSE)
## Now start the double bootstrap:
set.seed(41)
cover <- matrix(0, nrow = length(x.set), ncol = n.a)
covrS <- cover # for "percentile" CI
.CT <- system.time(
for(m in 1:M) {
cat(m,""); if(m %% 20 == 0) cat("\n")
i <- sample.int(n, replace = TRUE)
x. <- x[i]
y. <- y[i]
th. <- theta(x.,y., x.set)
th.. <- replicate(B, rTheta(x.,y., x.set)) ## -> matrix of dim  |x.set|  x  B
del.th <- th.. - th.
## all alpha quantiles (first all "lower", then all "upper");
## t(.): transpose to get  #{x.s} x #{alphas} matrix :
q.alph <- t(apply(del.th, 1, quantile,
prob = c(alphas/2, 1 - alphas/2), names = FALSE))
## "correct" (basic) bootstrap CIs:
thI <- th. - q.alph
lower <- thI[, n.a + 1:n.a, drop=FALSE]
upper <- thI[,       1:n.a, drop=FALSE]
cover <- cover + (lower <= th  & th <= upper)
## same for "simplistic percentile" CI
q.. <- q.alph + th. # == quantile(th.., alphas)
lower <- q..[,       1:n.a, drop=FALSE]
upper <- q..[, n.a + 1:n.a, drop=FALSE]
covrS <- covrS + (lower <= th  & th <= upper)
}); cat("\n")
.CT # 47 sec on fast lynne [i7, 2013]; 92 sec on nb-mm3 [lenovo X201]
cover <- cover / M
covrS <- covrS / M
dimnames(covrS) <- dimnames(cover) <-
list("x =" = formatC(x.set), "1-al. =" = formatC(1-alphas))
options(width=197); t(M*cover) # "M * . ": show the counts
## First simple plot (somewhat interesting for larger x.set):
pl.trajects <- function(alphas, cover, ylim = range(1-alphas, cover)) {
matplot(1 - alphas, t(cover), type = "b", ylim=ylim,
xlab = quote(1 - alpha), ylab = deparse(substitute(cover)), asp = 1)
abline(0,1, lty=3, col="gray50")
legend("topleft", paste("x=",x.set), col=1:6, lty=1:5,
pch = c(1L:9L, 0L, letters), bty="n")
}
yl <- range(1-alphas, cover, covrS)
pl.trajects(alphas, cover, ylim=yl)
pCIcorrect <- function(j, i.alpha0, alphas, cover, x.set, doMain=TRUE) {
p <- length(x.set)
stopifnot(p >= 1, dim(cover) == c(p, length(alphas)),
0 <= alphas, alphas <= 1,
0 <= cover, cover <= 1,
1 <= j, j <= p, length(i.alpha0) == 1)
## Explanation of double boot idea:
yl <- quote(P(theta %in% {I^{'*'}}(1-alpha)))
opar <- par(no.readonly = TRUE); on.exit(par(opar)) # reset graph.par at end
if(doMain) {
tit <- substitute(YL *":" ~~~ "correcting" ~alpha~~ "via double bootstrap",
list(YL = yl))
par(mgp = c(2.5, 1,0))
} else {
pm <- par("mar"); par(mar = c(pm[1], pm[2]+0.5, 0.5, pm[4]))
}
plot(1 - alphas, cover[j,], type = "o", # was "b", but we draw spline
col="red", # asp = 1,
ylim = range(1 - alphas, cover[j,]),
xlab = quote(1 - alpha), ylab = yl, main = if(doMain) tit)
if(doMain) mtext(bquote(x == .(x.set[j])))
abline(0,1, lty=3, col="gray50")
Pf <- splinefun(1-alphas, cover[j,], method="mono")# monotone spline
lines(i.alpha0, Pf(i.alpha0), type = "h", col=2, lty=2)
segments(i.alpha0, Pf(i.alpha0), i.alpha0, i.alpha0, col="green4")
text(i.alpha0, i.alpha0 +.01, quote(Delta[n]), col="green4", adj=-.1)
## an interpolator
curve(Pf, add=TRUE, col="red4")
text(1, Pf(1), quote({h^{'*'}}(1-alpha)), col="red4", pos = 3, xpd=NA)
## Compute the inverse h^{-1}(1-alpha) == 1-alpha' =: ialph.
ialph. <- uniroot(function(ia) Pf(ia) - i.alpha0, interval=c(0,1))$root
##        ^^^^^^^
segments(ialph., 0,     ialph., Pf(ialph.), col="blue4", lty=2)
segments(ialph., Pf(ialph.), 0, Pf(ialph.), col="blue4", lty=2)
mAxis <- function(labels=TRUE, line=-1, ...)
axis(1, at=round(ialph., 3), labels=labels,
col="blue4", col.axis="blue4", line=line, ...)
mAxis(); mAxis(labels = quote(1 - alpha*minute), line = -.25, tck=0)
}
j. <- which(x.set == -1)
j0 <- which(x.set == 0)
j1 <- which(x.set == 1)
pCIcorrect(j., 0.90, alphas, cover, x.set) # x.set[1] = -1
pCIcorrect(j0, 0.90, alphas, cover, x.set) # x.set[2] =  0
pCIcorrect(j1, 0.90, alphas, cover, x.set) # x.set[3] = +1
pCIcorrect(j., 0.90, alphas, covrS, x.set) # x.set[1] = -1
pCIcorrect(j0, 0.90, alphas, covrS, x.set) # x.set[2] =  0
pCIcorrect(j1, 0.90, alphas, covrS, x.set) # x.set[3] = +1
## at  x = 0 : -- try several alphas :
pCIcorrect(j0, 0.90, alphas, cover, x.set)
pCIcorrect(j0, 0.85, alphas, cover, x.set)
pCIcorrect(j0, 0.95, alphas, cover, x.set)
pCIcorrect(j0, 0.99, alphas, cover, x.set) # fails often [h(.) not invertible for large nominal 1-alpha
## x.set[3] = x = +1 --> less easy
pCIcorrect(j1, 0.95, alphas, cover, x.set) # using 0.99 outputs an error
pCIcorrect(j1, 0.90, alphas, cover, x.set)
pCIcorrect(j1, 0.85, alphas, cover, x.set)
pCIcorrect(j1, 0.80, alphas, cover, x.set)
plotCI_dBoot <- function(i.alpha0, cover, x.s = c(-1, 0, 1),
th = theta(x,y, x.set),
th.B = replicate(B, rTheta(x,y, x.set)),
doData = FALSE, addData=TRUE, doMain = TRUE,
col.cross = "red3", x.eq.y.line = TRUE,
marP = c(-1,-1, 0,0))
{
## Purpose: Do the Confidence Interval "double bootstrap correction"
## ----------------------------------------------------------------------
## Arguments:
## uses *global* variables  'x.set', 'alphas',
## 		 + if(doData)   'x', 'y', 'f()', 'rTheta()'
## 		 + if(doMain)   'M', 'B', 'n'
## ----------------------------------------------------------------------
## Author: Martin Maechler, Date: 11 May 2005
p <- length(x.s)
if(!identical(x.s, x.set)) { ## work on the subset  x.s  \in  x.set[]
j <- match(x.s, x.set); if(anyNA(j)) stop("some 'x.s' are not in 'x.set'")
x.set <- x.set[j]
cover <- cover[j,]
if(!missing(th.B)) th.B <- th.B[j, , drop=FALSE]
}
stopifnot(p >= 1,
dim(cover) == c(p, length(alphas)),
dim(th.B)  == c(p, B))
i.a <- 1 - alphas
if(is.unsorted(i.a)) stop("1- alphas must be sorted!")
i.a.P <- rep(NA, p)
mtit <- if(doMain)
bquote(atop("Bootstrap confidence interval correction",
list(1-alpha == .(i.alpha0), M == .(M),
B == .(B) * ",   sample size" ~ n == .(n))))
opl <- mult.fig(p + 1 + doData, main = mtit, marP=marP,
tit.wid = if(doMain) 3.25 else 0, line.main = 0.0)
on.exit( par(opl$old.par) )
if(doData)
plotData(x,y, x.set=x.set, x.bxp=FALSE, p.points = FALSE)
for(j in 1:p) {
c.prob <- cover[j,]
plot(i.a, c.prob, type = "b",
xlim = range(i.a), ylim = range(i.alpha0,c.prob), #  asp = 1,
main = paste("x = ", formatC(x.set[j])),
xlab = expression("nominal coverage  " * {1 - alpha}),
ylab = "estim. actual coverage")
if(x.eq.y.line)
abline(0,1, lty = 2, lwd = 0.5, col = "gray44")
usr <- par("usr")
can.corr <- (max(c.prob) >= i.alpha0)
if(can.corr) {
is.g <- c.prob >= i.alpha0
ii <- which(is.g)[1]        # the first one that is above
if(ii >= 2) {
f0 <- function(ia) splinefun(i.a, c.prob)(ia) - i.alpha0
iaI <- i.a[c(ii - 1, ii)]
if(any(i0 <- f0(iaI) == 0)) # catch exact zeros
i.a.P[j] <- mean(iaI[i0]) # in case there's > 1
else {
zf <- uniroot(f0, interval = iaI)
i.a.P[j] <- zf$root
}
cat("x=",x.set[j],": improved 1 - alpha' =",
formatC(i.a.P[j]), "\n")
segments(usr[1], i.alpha0,
i.a.P[j],  i.alpha0, col = col.cross)
segments(i.a.P[j],  usr[3],
i.a.P[j],  i.alpha0, col = col.cross, lty = 2)
mtext(formatC(i.a.P[j], dig=3), at = i.a.P[j],
side = 1, line = -1, cex = 0.8, col = col.cross)
} else ## i.a.P[j] remains NA
warning("all actual coverages are above", format(i.alpha0))
} else {
abline(h = i.alpha0, col = col.cross)
warning("x=",x.set[j],": All actual coverages are < ",
format(i.alpha0))
}
}
## Last (4-th) plot with the
## simple bootstrap and corrected bootstrap confidence intervals:
ci <- ciC <- matrix(NA, p, 2)
colnames(ci)  <- paste("simple", c("lo","up"), sep=".")
colnames(ciC) <- paste("double", c("lo","up"), sep=".")
a2 <- (1 - i.alpha0)/2 # = alpha / 2  (= 0.05 when i.a.. = 0.90)
for(j in 1:p) {
a2. <- (1 - i.a.P[j])/2 # = alpha' / 2
ci [j,] <- 2*th[j] - quantile(th.B[j,], c(1-a2,  a2))
ciC[j,] <- 2*th[j] - quantile(th.B[j,], c(1-a2., a2.)) # corrected
}
dx <- diff(rx <- range(x.set)); pp <- 0.1
xL <- rx+ c(-pp,pp)*dx
yL <- range(ci,ciC)
ylab <- expression(hat(theta) * " and C.I.")
main <- "simple and double bootstrap c.i."
if(addData)
plotData(x,y, x.set=x.set, x.bxp=FALSE, p.points = FALSE,
ylab=ylab, main=main, cex=1/2, col=adjustcolor(1, 1/2),
lwd.tr = 3/4, col.sm = adjustcolor(2, 4/5), lwd.sm = 1,
xlim = extendrange(xL, f=1/8), ylim = extendrange(yL, f=1/4))
(if(addData) points else plot)(
x.set, th, xlim = xL, ylim = yL, xlab="x", ylab=ylab, main=main,
pch = 4, cex = 0.75)
ciLine <- function(x, ci, ...)
arrows(x, ci[,1], x, ci[,2], angle=90, code=3, length=0.05, ...)
ciLine(x.set - pp/3*dx, ci, lwd=2)
ciLine(x.set + pp/3*dx, ciC, lwd=2.5, col = 2, lty = "longdash")
invisible(
cbind(x = x.set, i.alphaCorr = i.a.P, ci, ciC) )
}
plotCI_dBoot(i.alpha0 = 0.90, cover, th.B=th.B) # as in lecture notes
## other confidence levels alpha_0 :
try( plotCI_dBoot(i.alpha0 = 0.95, cover, th.B=th.B) )# --> Error  x=1   fails
try( plotCI_dBoot(i.alpha0 = 0.85, cover, th.B=th.B) )# ok {as 1-alpha --> to 0.76}
try( plotCI_dBoot(i.alpha0 = 0.88, cover, th.B=th.B) )# ok {standard ex}
try( plotCI_dBoot(i.alpha0 = 0.95, covrS, th.B=th.B) )# --> Error  x=1   fails
try( plotCI_dBoot(i.alpha0 = 0.85, covrS, th.B=th.B) )# ok {as 1-alpha --> to 0.76}
try( plotCI_dBoot(i.alpha0 = 0.88, covrS, th.B=th.B) )# ok {standard ex}
sess.info <- list(sessionInfo = sessionInfo(), Sys.info = Sys.info())
if(!interactive()) # not accidentally
save(theta, rTheta, f, plotData,
x,y, M,B,n, x.set, alphas, th.B,
cover, covrS, # <-
.CT, sess.info,
file = "doubleboot.rda")
rm(list = ls())
rm(list = ls())
diabetes <- read.table("http://stat.ethz.ch/Teaching/Datasets/diabetes2.dat",
header = TRUE)
reg <- diabetes[, c("Age", "C.Peptide")]
names(reg) <- c("x", "y")
reg <- reg[sort.list(reg$x), ]
loocv <- function(reg.data, reg.fcn)
{
## Help function to calculate leave-one-out regression values
loo.reg.value <- function(i, reg.data, reg.fcn)
return(reg.fcn(reg.data$x[-i], reg.data$y[-i], reg.data$x[i]))
## Calculate LOO regression values using the help function above
n <- nrow(reg.data)
loo.values <- sapply(1:n, loo.reg.value, reg.data, reg.fcn)
## Calculate and return MSE
mean((reg.data$y - loo.values)^2)
}
#We first plot the data, which is the same as in Figure 3.1 of the lecture notes. We guess a good
#bandwidth (h = 4), then define a regression function that can be used with loocv defined above.
par(pty="s")
plot(reg$x, reg$y, xlab = "Age", ylab = "Log-concentration")
h <- 4
reg.fcn.nw <- function(reg.x, reg.y, x)
ksmooth(reg.x, reg.y, x.points = x, kernel = "normal", bandwidth = h)$y
(cv.nw <- loocv(reg, reg.fcn.nw))
#We calculate the hat matrix ”manually” in order to calculate the degrees of freedom; this is the
#smoothing parameter used for other regression estimators:
n <- nrow(reg)
Id <- diag(n)
S.nw <- matrix(0, n, n)
for (j in 1:n)
S.nw[, j] <- reg.fcn.nw(reg$x, Id[, j], reg$x)
(df.nw <- sum(diag(S.nw)))
Id
View(Id[,2])
(Id[,2])
S.nw
View(S.nw)
S.nw %*% reg$y
S.nw %*% reg$y - reg.fcn.nw(reg$x, reg$y, reg$x)
#We also do the calculation of the CV value with the hat matrix:
y.fit.nw <- reg.fcn.nw(reg$x, reg$y, reg$x)
(cv.nw.hat <- mean(((reg$y - y.fit.nw)/(1 - diag(S.nw)))^2))
(cv.nw <- loocv(reg, reg.fcn.nw))
#Moreover, we can also simply use hatMat from the package sfsmisc:
library(sfsmisc)
#degrees of freedom
hatMat(reg$x,trace=TRUE,pred.sm=reg.fcn.nw,x=reg$x)
#Local polynomial (“lp”) regression from loess:
reg.fcn.lp <- function(reg.x, reg.y, x) {
lp.reg <- loess(reg.y ~ reg.x, enp.target = df.nw, surface = "direct")
predict(lp.reg, x)
}
(cv.lp <- loocv(reg, reg.fcn.lp))
more numerical predictors, u
#Again, we also calculate the CV value with the hat matrix constructed “manually”:
n <- nrow(reg)
more numerical predictors, u
Id <- diag(n)
more numerical predictors, u
S.lp <- matrix(0, n, n)
#Again, we also calculate the CV value with the hat matrix constructed “manually”:
n <- nrow(reg)
Id <- diag(n)
S.lp <- matrix(0, n, n)
for (j in 1:n)
S.lp[, j] <- reg.fcn.lp(reg$x, Id[, j], reg$x)
y.fit.lp <- reg.fcn.lp(reg$x, reg$y, reg$x)
(cv.lp.hat <- mean(((reg$y - y.fit.lp)/(1 - diag(S.lp)))^2))
#And once more, we also compute the CV value using hatMat:
S.lp.hatMat <- hatMat(reg$x,trace=FALSE,pred.sm=reg.fcn.lp,x=reg$x)
(cv.lp.hatMat <- mean(((reg$y - y.fit.lp)/(1 - diag(S.lp.hatMat)))^2))
est.ss <- smooth.spline(reg$x, reg$y, cv = TRUE, df = df.nw)
est.ss$cv.crit
est.ss$cv
#We then use the same smoothing parameter spar for our own calculations of the CV value:
reg.fcn.ss <- function(reg.x, reg.y, x)
{
ss.reg <- smooth.spline(reg.x, reg.y, spar = est.ss$spar)
predict(ss.reg, x)$y
}
(cv.ss <- loocv(reg, reg.fcn.ss))
est.ss$cv.crit
(cv.ss <- loocv(reg, reg.fcn.ss))
#Alternative calculation using the hat-matrix computed “manually”:
n <- nrow(reg)
Id <- diag(n)
S.ss <- matrix(0, n, n)
for (j in 1:n)
S.ss[, j] <- reg.fcn.ss(reg$x, Id[, j], reg$x)
y.fit.ss <- reg.fcn.ss(reg$x, reg$y, reg$x)
(cv.ss.hat <- mean(((reg$y - y.fit.ss)/(1 - diag(S.ss)))^2))
#And alternative calculation using hatMat:
S.ss.hatMat <- hatMat(reg$x,trace=FALSE,pred.sm=reg.fcn.ss,x=reg$x)
(cv.ss.hatMat <- mean(((reg$y - y.fit.ss)/(1 - diag(S.ss.hatMat)))^2))
#Smoothing spline regression with optimized degrees of freedom:
est.ssopt <- smooth.spline(reg$x, reg$y, cv = TRUE)
cv.ssopt <- est.ssopt$cv.crit
(cv.ssopt)
(est.ssopt$df)
Constant fit (“CF”):
#Constant fit (“CF”):
reg.fcn.cf <- function(reg.x, reg.y, x) mean(reg.y)
(cv.cf <- loocv(reg, reg.fcn.cf))
sort(c(nw=cv.nw,lp=cv.lp,ss=cv.ss,ssopt=cv.ssopt,cf=cv.cf))
plot(x, y, data = reg)
?plot
plot(data$x, data$y)
plot(reg$x, reg$y)
plot(est.ssopt$y)
plot(reg$x, reg$y)
lines(est.ssopt$x, est.ssopt$y, col = 2)
lines(est.ss$x, est.ss$y, col = 2)
plot(reg$x, reg$y)
lines(est.ssopt$x, est.ssopt$y, col = 2)
lines(est.ss$x, est.ss$y, col = 3)
lines(est.ss$x, y.fit.lp, col = 4)
lines(reg$x, y.fit.lp, col = 4)
lines(reg$x, y.fit.lp, col = 4)
plot(reg$x, reg$y)
lines(est.ssopt$x, est.ssopt$y, col = 2)
lines(est.ss$x, est.ss$y, col = 3)
lines(reg$x, y.fit.lp, col = 4)
plot(reg$x, reg$y, ylab = 'Response',  x = 'Covariates')
lines(est.ssopt$x, est.ssopt$y, col = 2)
lines(est.ss$x, est.ss$y, col = 3)
lines(reg$x, y.fit.lp, col = 4)
plot(reg$x, reg$y, ylab = 'Response',  x = 'Covariates')
plot(reg$x, reg$y, ylab = 'Response',  xlab = 'Covariates')
lines(est.ssopt$x, est.ssopt$y, col = 2)
lines(est.ss$x, est.ss$y, col = 3)
lines(reg$x, y.fit.lp, col = 4)
lines(est.ssopt$x, est.ssopt$y, col = 2, pch = 2)
lines(est.ssopt$x, est.ssopt$y, col = 2, pch = 16)
lines(est.ssopt$x, est.ssopt$y, col = 2, lty = 16)
lines(est.ssopt$x, est.ssopt$y, col = 2, lty = 4)
lines(est.ssopt$x, est.ssopt$y, col = 2, lwd = 4)
plot(reg$x, reg$y, ylab = 'Response',  xlab = 'Covariates')
lines(est.ssopt$x, est.ssopt$y, col = 2, lwd = 2)
lines(est.ss$x, est.ss$y, col = 3)
lines(reg$x, y.fit.lp, col = 4)
plot(reg$x, reg$y, ylab = 'Response',  xlab = 'Covariates')
lines(est.ssopt$x, est.ssopt$y, col = 2, lwd = 2)
lines(est.ss$x, est.ss$y, col = 3, lwd = 2)
lines(reg$x, y.fit.lp, col = 4, lwd = 2)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggsci)
library(scales)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Default theme for charts
theme_set(
theme_bw() +
theme(
plot.title = element_text(face = "bold", size = 14),
plot.subtitle = element_text(
face = "italic", size = 10, colour = "green50"
)
)
)
# Read data - both direction JAO
df <- read_csv("../00 Data Retrieval and Cleaning/0_df_final_imputed.csv")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggsci)
library(scales)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Default theme for charts
theme_set(
theme_bw() +
theme(
plot.title = element_text(face = "bold", size = 14),
plot.subtitle = element_text(
face = "italic", size = 10, colour = "grey50"
)
)
)
# Read data - both direction JAO
data_ch_de = read_csv("../00 Data Retrieval and Cleaning/0_df_final_ch-de_UTC.csv")
data_de_ch = read_csv("../00 Data Retrieval and Cleaning/0_df_final_de-ch_UTC.csv")
data_ch_de <- data_ch_de %>%
rename(auction_price_ch_de = auction_price,
allocatedCapacity_ch_de = allocatedCapacity, ATC_ch_de = ATC)
#Initialize df that contain ALL variables
df <- data_ch_de %>%
mutate(auction_price_de_ch = data_de_ch$auction_price,
allocatedCapacity_de_ch= data_de_ch$allocatedCapacity,
ATC_de_ch= data_de_ch$ATC) %>%
arrange(date)
rm(data_ch_de, data_de_ch)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggsci)
library(scales)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Default theme for charts
theme_set(
theme_bw() +
theme(
plot.title = element_text(face = "bold", size = 14),
plot.subtitle = element_text(
face = "italic", size = 10, colour = "grey50"
)
)
)
