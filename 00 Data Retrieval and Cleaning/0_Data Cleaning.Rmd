---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ggsci)
library(scales)

setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

# Default theme for charts
theme_set(
  theme_bw() +
    theme(  
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(
        face = "italic", size = 10, colour = "grey50"
      )
    )
)

```

## The Goal

The goal of this document is to 1) select variables based on availability (missing values) and variance (remove zero-variance) and 2) ensure conversion to the appropriate data type (we have numerical data only, however categories, i.e. integer variables could be represented as factors).

### Combining the two separate files

```{r}
# Read data - both direction JAO
data_ch_de = read_csv("../00 Data Retrieval and Cleaning/0_df_final_ch-de_UTC.csv")
data_de_ch = read_csv("../00 Data Retrieval and Cleaning/0_df_final_de-ch_UTC.csv")

data_ch_de <- data_ch_de %>%
  rename(auction_price_ch_de = auction_price,
         allocatedCapacity_ch_de = allocatedCapacity, ATC_ch_de = ATC)

#Initialize df that contain ALL variables
df <- data_ch_de %>% 
  mutate(auction_price_de_ch = data_de_ch$auction_price, 
         allocatedCapacity_de_ch = data_de_ch$allocatedCapacity, 
         ATC_de_ch = data_de_ch$ATC) %>% 
  arrange(date)

rm(data_ch_de, data_de_ch)
```

### Removing missing values at the end of the dataframe

Remove the most recent rows that contains NA:

```{r}
split_date = ymd_hm("2024-01-31 23:00")
df <- df %>% 
  filter(date < split_date)
```

### Removing zero-variance predictors

Find and remove zero variance variables:

```{r}
remove_cols = df |> 
  select(-c(date)) |> 
  pivot_longer(everything()) |> 
  group_by(name) |> 
  summarise(var = var(value, na.rm = T)) |> 
  ungroup() |> 
  filter(var < 1e-10) |> 
  pull(name)

df = df |> 
  select(!all_of(remove_cols))
```

### Removing columns that have too many missing values

How many columns have more than x% missing data?

```{r}
tibble(
  percentage = seq(0.01, 0.99, by = 0.01),
  cols = map_dbl(percentage, function(pct){
    df |> 
      select(
        colMeans(is.na(df)) |> 
          enframe() |>
          filter(value > pct) |> 
          pull(name) 
      ) |> 
      ncol()
  })
) |> 
  ggplot(aes(percentage, cols)) +
  geom_line() +
  geom_point() +
  labs(title = "Number of columns with more than x% missing values",
       y = "# of cols", x = "Minimum Missing Percentage") +
  scale_x_continuous(breaks = seq(0, 1, 0.1), 
                     labels = percent_format())
```

We decided to limit ourselves to discard columns with more than 60% of missing data:

```{r}
# Discard the columns with too many missing values (threshold 60%)
df = df |> 
  select(-(df |> 
             is.na() |> 
             colMeans() |> 
             enframe() |> 
             filter(value > 0.6) |> 
             pull(name)))
```

### French Pumped Storage

These two columns are not missing at random, they're pumped storage values showing production and consumption, fill the NA values with zero and keep both columns:

```{r}
df |> 
  select(hydro_pumped_storage_actual_aggregated_fr,
         hydro_pumped_storage_actual_consumption_fr) |> 
  is.na() |> 
  colMeans()

df |> 
  select(hydro_pumped_storage_actual_aggregated_fr,
         hydro_pumped_storage_actual_consumption_fr) |> 
  mutate(tmp = xor(is.na(hydro_pumped_storage_actual_aggregated_fr),
                   is.na(hydro_pumped_storage_actual_consumption_fr))) |> 
  summarise(xor_missing = mean(tmp))

df |> 
  slice(5200:5300) |> 
  select(date, 
         hydro_pumped_storage_actual_aggregated_fr,
         hydro_pumped_storage_actual_consumption_fr) |> 
  pivot_longer(-date) |> 
  ggplot(aes(date, value, colour = name)) +
  geom_line()

df = df |> 
  mutate(
    hydro_pumped_storage_actual_aggregated_fr = ifelse(
      is.na(hydro_pumped_storage_actual_aggregated_fr),
      0,
      hydro_pumped_storage_actual_aggregated_fr
    ),
    hydro_pumped_storage_actual_consumption_fr = ifelse(
      is.na(hydro_pumped_storage_actual_consumption_fr),
      0,
      hydro_pumped_storage_actual_consumption_fr
    )
  )
```

Let's visualise the missingness in the rest of the variables:

```{r}
df |> 
  naniar::vis_miss(warn_large_data = F)
```

### Forecast variables

Remove forecast values as we are using the actuals instead as discussed in the meeting with the supervisor.

There are columns that contains forecast and other that contains the actual value.

We decided to use keep only the actual data for those variables that also have a forecast for, since we can not trace back when the forecast was received by the traders.

```{r}
# Identify columns containing "actual" and "forecast" using grep
actual_columns <- grep("actual", names(df), value = TRUE)
forecast_columns <- grep("forecast", names(df), value = TRUE)
print(actual_columns)
print('')
print(forecast_columns)
```

The solar_forecast_country and the solar_actual_aggregated_country express similar quantities: the former is a forecast of wind and solar power generation as an average of forecasted power output per Market Time Unit and per bidding zone, while the latter expresses the actual aggregated output per Market Time Unit (sum). Since we cannot recover the exact time when the forecasts are published (when the forecasts are available to traders), and we can assume that the forecasts nowadays are accurate, we decided to remove the forecasts. This reasoning is also applied to other variables.

```{r}
remove_forecast <- c('solar_forecast_fr', 'solar_forecast_ch', 'solar_forecast_it','solar_forecast_at', 'wind_offshore_forecast_de', 'wind_onshore_forecast_de', 'wind_onshore_forecast_ch', 'wind_onshore_forecast_at')

df <- df %>% 
  select(!all_of(remove_forecast))
```

### Reorder and rename some variables to make it more understandable

```{r}
reorder_cols = c("date", "auction_price_ch_de", "auction_price_de_ch",
                 "allocatedCapacity_ch_de", "allocatedCapacity_de_ch",
                 "ATC_ch_de", "ATC_de_ch")

df = bind_cols(
  df |> select(any_of(reorder_cols)),
  df |> select(-any_of(reorder_cols))
)
```

Write the non-imputed dataset to csv:

```{r}
df |> 
  write_csv("../00 Data Retrieval and Cleaning/0_df_final_not_imputed.csv")
```

### Imputation with Zero and Include Dummies for Missing

Are there missing values in the target variable?

```{r}
df |> 
  select(contains("auction")) |> 
  is.na() |> 
  colSums()
```

There are no missing values in the target variable.

The imputation method used for this project after discussion with the supervisor is to fill missing values with zero and include a dummy variable indicating for each column with missing data which indicates whether data in this corresponding row is missing.


```{r}
df2 <- df #with missing INITIALIZE

df3 <- as.matrix(df2)
df4 <- df2 %>% 
  select(missing_cols) %>% 
  as.matrix()
```



```{r}
# Identify columns with missing values
missing_cols = df |> 
  is.na() |> 
  colSums() |> 
  enframe() |> 
  filter(value != 0) |> 
  pull(name)

# Create three different dataframes of same row length and combine them 
# to the final data frame again
# 1: Columns that have no missing values
# 2: Columns that have missing values filled with zeros where missing
# 3: Columns that have missing values transformed to dummies

df = bind_cols(
  # 1
  df |> 
    select(-any_of(missing_cols)),
  # 2
  df |> 
    select(any_of(missing_cols)) |> 
    mutate(id = 1:n()) |> 
    pivot_longer(-id) |> 
    group_by(name) |> 
    mutate(value = ifelse(is.na(value), 0, value)) |> 
    ungroup() |> 
    pivot_wider(names_from = name, values_from = value) |> 
    select(-id),
  # 3
  df |> 
    select(any_of(missing_cols)) |> 
    mutate(id = 1:n()) |> 
    pivot_longer(-id) |> 
    group_by(name) |> 
    mutate(value = ifelse(is.na(value), 1, 0)) |> 
    ungroup() |> 
    mutate(name = paste0(name, "_missing_dummy")) |> 
    pivot_wider(names_from = name, values_from = value) |> 
    select(-id)
)
```

```{r}
df |> 
  is.na() |> 
  colSums() |> 
  enframe() |> 
  filter(value != 0)
```

There are no more columns with missing values left!

```{r}
df |> 
  write_csv("../00 Data Retrieval and Cleaning/0_df_final_imputed.csv")
```

Save the dataset in a file so we can use it in the future with the same imputation.

# Check imputation 


```{r}
cond <- is.na(df3)
df_no_dummy <- df %>% 
  select(colnames(df2))

df_no_dummy <- data.matrix(df_no_dummy)
df_no_dummy[cond] %>% sum() #ALL the NA where replaced by 0 in df

missing_counts <- df2 %>%
  select(missing_cols) %>% 
  summarise_all(~ sum(is.na(.)))



cond <- is.na(df4) #matrix of dummy
df_dummy <- df %>% 
  select(!colnames(df2))

df_dummy[!cond] %>% sum() #All the non-NA where replaced by 0
tib <- df_dummy %>% colSums() %>% tibble()
tib2 <- t(missing_counts) %>% tibble()
tib - tib2 #Also the umber of missing vaues coincide
```


# Observation from dummies column

After checking that the imputation was correct notice taht same column of the the missing values coincide  with each other.We decide to identify this column and use only one dummy.


```{r}
#If the columns are identical they must have same mean
mn <- colMeans(df_dummy)
mn <- mn[duplicated(mn)] #choose duplicated

dup <- unique(mn[duplicated(mn)])
cond <- mn %in% dup 
mn <- mn[cond]
tib <- tibble(names = names(mn), values = mn)

# Group by value and extract names
same_value_names <- tib %>%
  group_by(values) %>%
  summarise(names = list(names)) %>%
  filter(length(names) > 1) %>%
  pull(names)

#Count the number of 1 per col
df_dummy %>% 
  select(same_value_names[[1]]) %>% 
  colSums()

#Count the values per row. If all 1 then they sum up to 31
row <- df_dummy %>% 
  select(same_value_names[[1]]) %>% 
  rowSums()

same_value_names[[1]] %>% length

(row == 32) %>% sum #The number of row == 32 should be equal to the number of 1 per columns -- > same df

#REPEAT
df_dummy %>% 
  select(same_value_names[[2]]) %>% 
  colSums()

row <- df_dummy %>% 
  select(same_value_names[[2]]) %>% 
  rowSums()

same_value_names[[2]] %>% length

(row == 8) %>% sum


df_dummy %>% 
  select(same_value_names[[3]]) %>% 
  colSums()

row <- df_dummy %>% 
  select(same_value_names[[3]]) %>% 
  rowSums()

same_value_names[[3]] %>% length

(row == 2) %>% sum   ###FAILED


df_dummy %>% 
  select(same_value_names[[4]]) %>% 
  colSums()

row <- df_dummy %>% 
  select(same_value_names[[4]]) %>% 
  rowSums()

same_value_names[[4]] %>% length

(row == 2) %>% sum
```

Having check that the imputation is correct we can say that: except for the third group, all the columns inside the same group share the same dummy 'distribution'. So we have to think of a depence btw this variable and how to procede with the modelling. It's true that we introduce multicollinearity but the single dummy should represent different effect, so how to procede? Should we reduce the dimension of dummy matrix or keep as it is now?

The first group considers:
capacity and alllcation of capacity it make sense that prset the same missing but is not clear how they are dependent with the otehrs.

The second group is dominanted by _fr variable so this can be teh source of dependence. 

The fourth groupd is the same variable hydro_reservoir_storage for _at and _ch





