---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ggsci)
library(scales)

setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

# Default theme for charts
theme_set(
  theme_bw() +
    theme(  
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(
        face = "italic", size = 10, colour = "grey50"
      )
    )
)

```

## The Goal

The goal of this document is to 1) select variables based on availability (missing values) and variance (remove zero-variance) and 2) ensure conversion to the appropriate data type (we have numerical data only, however categories, i.e. integer variables could be represented as factors).

### Combining the two separate files

```{r}
# Read data - both direction JAO
data_ch_de = read_csv("../00 Data Retrieval and Cleaning/0_df_final_ch-de_UTC.csv")
data_de_ch = read_csv("../00 Data Retrieval and Cleaning/0_df_final_de-ch_UTC.csv")

data_ch_de <- data_ch_de %>%
  rename(auction_price_ch_de = auction_price,
         allocatedCapacity_ch_de = allocatedCapacity, ATC_ch_de = ATC)

#Initialize df that contain ALL variables
df <- data_ch_de %>% 
  mutate(auction_price_de_ch = data_de_ch$auction_price, 
         allocatedCapacity_de_ch= data_de_ch$allocatedCapacity, 
         ATC_de_ch= data_de_ch$ATC) %>% 
  arrange(date)

rm(data_ch_de, data_de_ch)
```

### Removing missing values at the end of the dataframe

Remove the most recent rows that contains NA:

```{r}
split_date = ymd_hm("2024-01-31 23:00")
df <- df %>% 
  filter(date < split_date)
```

### Removing zero-variance predictors

Find and remove zero variance variables:

```{r}
remove_cols = df |> 
  select(-c(date)) |> 
  pivot_longer(everything()) |> 
  group_by(name) |> 
  summarise(var = var(value, na.rm = T)) |> 
  ungroup() |> 
  filter(var < 1e-10) |> 
  pull(name)

df = df |> 
  select(!all_of(remove_cols))
```

### Removing columns that have too many missing values

How many columns have more than x% missing data?

```{r}
tibble(
  percentage = seq(0.01, 0.99, by = 0.01),
  cols = map_dbl(percentage, function(pct){
    df |> 
      select(
        colMeans(is.na(df)) |> 
          enframe() |>
          filter(value > pct) |> 
          pull(name) 
      ) |> 
      ncol()
  })
) |> 
  ggplot(aes(percentage, cols)) +
  geom_line() +
  geom_point() +
  labs(title = "Number of columns with more than x% missing values",
       y = "# of cols", x = "Minimum Missing Percentage") +
  scale_x_continuous(breaks = seq(0, 1, 0.1), 
                     labels = percent_format())
```

We decided to limit ourselves to discard columns with more than 60% of missing data:

```{r}
# Discard the columns with too many missing values (threshold 60%)
df = df |> 
  select(-(df |> 
             is.na() |> 
             colMeans() |> 
             enframe() |> 
             filter(value > 0.6) |> 
             pull(name)))
```

### French Pumped Storage

These two columns are not missing at random, they're pumped storage values showing production and consumption, fill the NA values with zero and keep both columns:

```{r}
df |> 
  select(hydro_pumped_storage_actual_aggregated_fr,
         hydro_pumped_storage_actual_consumption_fr) |> 
  is.na() |> 
  colMeans()

df |> 
  select(hydro_pumped_storage_actual_aggregated_fr,
         hydro_pumped_storage_actual_consumption_fr) |> 
  mutate(tmp = xor(is.na(hydro_pumped_storage_actual_aggregated_fr),
                   is.na(hydro_pumped_storage_actual_consumption_fr))) |> 
  summarise(xor_missing = mean(tmp))

df |> 
  slice(5200:5300) |> 
  select(date, 
         hydro_pumped_storage_actual_aggregated_fr,
         hydro_pumped_storage_actual_consumption_fr) |> 
  pivot_longer(-date) |> 
  ggplot(aes(date, value, colour = name)) +
  geom_line()

df = df |> 
  mutate(
    hydro_pumped_storage_actual_aggregated_fr = ifelse(
      is.na(hydro_pumped_storage_actual_aggregated_fr),
      0,
      hydro_pumped_storage_actual_aggregated_fr
    ),
    hydro_pumped_storage_actual_consumption_fr = ifelse(
      is.na(hydro_pumped_storage_actual_consumption_fr),
      0,
      hydro_pumped_storage_actual_consumption_fr
    )
  )
```

Let's visualise the missingness in the rest of the variables:

```{r}
df |> 
  naniar::vis_miss(warn_large_data = F)
```

### Forecast variables

Remove forecast values as we are using the actuals instead as discussed in the meeting with the supervisor.

There are columns that contains forecast and other that contains the actual value.

We decided to use keep only the actual data for those variables that also have a forecast for, since we can not trace back when the forecast was received by the traders.

```{r}
# Identify columns containing "actual" and "forecast" using grep
actual_columns <- grep("actual", names(df), value = TRUE)
forecast_columns <- grep("forecast", names(df), value = TRUE)
print(actual_columns)
print('')
print(forecast_columns)
```

The solar_forecast_country and the solar_actual_aggregated_country express similar quantities: the former is a forecast of wind and solar power generation as an average of forecasted power output per Market Time Unit and per bidding zone, while the latter expresses the actual aggregated output per Market Time Unit (sum). Since we cannot recover the exact time when the forecasts are published (when the forecasts are available to traders), and we can assume that the forecasts nowadays are accurate, we decided to remove the forecasts. This reasoning is also applied to other variables.

```{r}
remove_forecast <- c('solar_forecast_fr', 'solar_forecast_ch', 'solar_forecast_it','solar_forecast_at', 'wind_offshore_forecast_de', 'wind_onshore_forecast_de', 'wind_onshore_forecast_ch', 'wind_onshore_forecast_at')

df <- df %>% 
  select(!all_of(remove_forecast))
```

### Reorder and rename some variables to make it more understandable

TBD

```{r}
reorder_cols = c("date", "auction_price_ch_de", "auction_price_de_ch",
                 "allocatedCapacity_ch_de", "allocatedCapacity_de_ch",
                 "ATC_ch_de", "ATC_de_ch")

df = bind_cols(
  df |> select(any_of(reorder_cols)),
  df |> select(-any_of(reorder_cols))
)
```

Write the non-imputed dataset to csv:

```{r}
df |> 
  write_csv("../00 Data Retrieval and Cleaning/0_df_final_not_imputed.csv")
```

### Imputation with Zero and Include Dummies for Missing

Are there missing values in the target variable?

```{r}
df |> 
  select(contains("auction")) |> 
  is.na() |> 
  colSums()
```

There are no missing values in the target variable.

The imputation method used for this project after discussion with the supervisor is to fill missing values with zero and include a dummy variable indicating for each column with missing data which indicates whether data in this corresponding row is missing.

```{r}
# Identify columns with missing values
missing_cols = df |> 
  is.na() |> 
  colSums() |> 
  enframe() |> 
  filter(value != 0) |> 
  pull(name)

# Create three different dataframes of same row length and combine them 
# to the final data frame again
# 1: Columns that have no missing values
# 2: Columns that have missing values filled with zeros where missing
# 3: Columns that have missing values transformed to dummies

df = bind_cols(
  # 1
  df |> 
    select(-any_of(missing_cols)),
  # 2
  df |> 
    select(any_of(missing_cols)) |> 
    mutate(id = 1:n()) |> 
    pivot_longer(-id) |> 
    group_by(name) |> 
    mutate(value = ifelse(is.na(value), 0, value)) |> 
    ungroup() |> 
    pivot_wider(names_from = name, values_from = value) |> 
    select(-id),
  # 3
  df |> 
    select(any_of(missing_cols)) |> 
    mutate(id = 1:n()) |> 
    pivot_longer(-id) |> 
    group_by(name) |> 
    mutate(value = ifelse(is.na(value), 1, 0)) |> 
    ungroup() |> 
    mutate(name = paste0(name, "_missing_dummy")) |> 
    pivot_wider(names_from = name, values_from = value) |> 
    select(-id)
)
```

```{r}
df |> 
  is.na() |> 
  colSums() |> 
  enframe() |> 
  filter(value != 0)
```

There are no more columns with missing values left!

```{r}
df |> 
  write_csv("../00 Data Retrieval and Cleaning/0_df_final_imputed.csv")
```

Save the dataset in a file so we can use it in the future with the same imputation.