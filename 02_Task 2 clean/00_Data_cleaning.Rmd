---
title: "Data cleaning"
author: "Federico Deotto"
date: "2024-03-12"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ggsci)
library(scales)
# library(tidymodels)

setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

# Default theme for charts
theme_set(
  theme_bw() +
    theme(  
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(
        face = "italic", size = 10, colour = "grey50"
      )
    )
)

```

## The Goal

Select variables based on availability (NA) and variance, ensuring conversion to the appropriate type (e.g., converting integers into numeric data).

```{r}
# Read data - both direction JAO
data_ch_de = read_csv("../00_Data Retrieval and Cleaning/0_df_final_ch-de_UTC.csv")
data_de_ch = read_csv("../00_Data Retrieval and Cleaning/0_df_final_de-ch_UTC.csv")

data_ch_de <- data_ch_de %>%
  rename(auction_price_ch_de = auction_price,
         allocatedCapacity_ch_de = allocatedCapacity, ATC_ch_de = ATC)

#Initialize df that contain ALL variables
df <- data_ch_de %>% 
  mutate(auction_price_de_ch = data_de_ch$auction_price, 
         allocatedCapacity_de_ch= data_de_ch$allocatedCapacity, 
         ATC_de_ch= data_de_ch$ATC) %>% 
  select(-dst) %>% 
  arrange(date)

rm(data_ch_de, data_de_ch)
```


Remove the most recent rows that contains NA:

```{r}
split_date = ymd_hm("2024-01-31 23:00")
df <- df %>% 
  filter(date < split_date)
```


Find and remove zero variance variables:

```{r}
remove_cols = df |> 
  select(-c(date)) |> 
  pivot_longer(everything()) |> 
  group_by(name) |> 
  summarise(var = var(value, na.rm = T)) |> 
  ungroup() |> 
  filter(var < 1e-10) |> 
  pull(name)

df = df |> 
  select(!all_of(remove_cols))
```

### Missing value

How many columns have more than x% missing data?

```{r}
tibble(
  percentage = seq(0.01, 0.99, by = 0.01),
  cols = map_dbl(percentage, function(pct){
    df |> 
      select(
        colMeans(is.na(df)) |> 
          enframe() |>
          filter(value > pct) |> 
          pull(name) 
      ) |> 
      ncol()
  })
) |> 
  ggplot(aes(percentage, cols)) +
  geom_line() +
  geom_point() +
  labs(title = "Number of columns with more than x% missing values",
       y = "# of cols", x = "Minimum Missing Percentage") +
  scale_x_continuous(breaks = seq(0, 1, 0.1), 
                     labels = percent_format())
```

Agree with M. to limit ourselves with the 30% (/10%)

```{r}
df = df |> 
  select(
    colMeans(is.na(df)) |> 
      enframe() |>
      filter(value < 0.3) |> 
      pull(name) 
  )
```

Visualise the missingness in those columns that still have missing values:

```{r}
df |> 
  select(
    colMeans(is.na(df)) |> 
      enframe() |>
      filter(value > 0) |> 
      pull(name)
  ) |> 
  naniar::vis_miss(warn_large_data = F)
```

Remove forecast values as we are using the actuals instead:

Before imputation remove other variables based on following:

There are columns that contains forecast and other that contains the actual value.
We decided to use keep only the actual data for those variables that also have a forecast, since we can not trace back when the forecast was received by the traders.

```{r}
# Identify columns containing "actual" and "forecast" using grep
actual_columns <- grep("actual", names(df), value = TRUE)
forecast_columns <- grep("forecast", names(df), value = TRUE)
print(actual_columns)
print('')
print(forecast_columns)
```

The solar_forecast_country and the solar_actual_aggregated_country express similar quantities: the former is a forecast of wind and solar power generation as an average of forecasted power output per Market Time Unit and per bidding zone, while the latter expresses the actual aggregated output per Market Time Unit (sum). Since we cannot recover the exact time when the forecasts are published (when the forecasts are available to traders), and we can assume that the forecasts nowadays are accurate, we decided to remove the forecasts. This reasoning is also applied to other variables.

```{r}
remove_forecast <- c('solar_forecast_fr', 'solar_forecast_ch', 'solar_forecast_it','solar_forecast_at', 'wind_offshore_forecast_de', 'wind_onshore_forecast_de', 'wind_onshore_forecast_ch', 'wind_onshore_forecast_at')

df <- df %>% 
  select(!all_of(remove_forecast))
```

Impute the missing values with Random Forest:

```{r, results=FALSE, eval=FALSE}
set.seed(1234)
df_imputed = missRanger::missRanger(
  df,
  num.trees = 3,
  sample.fraction = 0.1,
  verbose = 1
)
```

Save the dataset in a file so we can use it in the future with the same imputation.

```{r, eval=FALSE}
write.csv(df_imputed, "data_cleaned_UTC.csv", row.names = FALSE)
```