---
title: "Untitled"
author: "Mathias Steilen"
date: "2024-04-25"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(xgboost)
library(doParallel)
library(ggsci)
library(scales)
library(vip)

setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

# Default theme for charts
theme_set(
  theme_bw() +
    theme(  
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(
        face = "italic", size = 10, colour = "green50"
      )
    )
)
```

```{r}
df = read_csv("../00 Data Retrieval and Cleaning/0_df_final_imputed_shifting_a.csv")
```

### Determine the Splits of the 3 regimes

```{r}
df |> 
  select(date, contains("day_ahead_price")) |> 
  pivot_longer(-date) |> 
  ggplot(aes(x = date, y = value, colour = name)) +
  geom_line() +
  facet_wrap(~ name, ncol = 1, scales = "free") +
  scale_x_datetime(date_breaks = "6 months", date_labels = "%m-%Y") +
  theme(legend.position = "none")
```

Splits that make sense:

First: < 06/21 (Test: 70/30 split)
Second: >= 06/21 & < 03/23 (Test: 70/30 split)
Third: >= 03/23 (Test: 70/30 split)
Fourth: everything (Test: > 09/23)

```{r}
df |> 
  select(date, contains("auction")) |> 
  pivot_longer(-date) |> 
  ggplot(aes(x = date, y = value, colour = name)) +
  geom_line() +
  facet_wrap(~ name, ncol = 1, scales = "free") +
  scale_x_datetime(date_breaks = "6 months", date_labels = "%m-%Y") +
  theme(legend.position = "none")
```

Splits that make sense for JAO:

First: 



### Drop columns that can't be used

```{r}
df = df |> 
  select(-c(
    auction_price_ch_de, allocatedCapacity_ch_de
  ))
```


### Shift the other target variables

Anything that is a target variable in our project has not been shifted yet.

Anything that is not a target variable in some project is shifted according to the A, B, C logic that we discussed.

```{r}
df = df |> 
  mutate(
    # auction_price_ch_de = lag(auction_price_ch_de, n = 24),
    auction_price_de_ch = lag(auction_price_de_ch, n = 0),
    # allocatedCapacity_ch_de = lag(allocatedCapacity_ch_de, n = 24),
    allocatedCapacity_de_ch = lag(allocatedCapacity_de_ch, n = 24),
    day_ahead_price_at = lag(day_ahead_price_at, n = 24),
    day_ahead_price_ch = lag(day_ahead_price_ch, n = 24),
    day_ahead_price_de = lag(day_ahead_price_de, n = 24),
    day_ahead_price_fr = lag(day_ahead_price_fr, n = 24),
    day_ahead_price_it = lag(day_ahead_price_it, n = 24),
  ) |> 
  drop_na()

df |> glimpse()
```

### Splitting the Data and Creating Folds for Cross Validation

```{r}
dt_train = df |> filter(year(date) < 2023)
dt_test = df |> filter(year(date) >= 2023)
```

```{r}
# How complicated can this be
# Who invented this
initial_split = 0.5
increment = 0.1
train_perc = 0.8

initial_train = floor(nrow(dt_train)*(initial_split+increment)*train_perc)
initial_test = round(floor(nrow(dt_train) * initial_split * (1-train_perc)))

folds = rolling_origin(
  dt_train, 
  initial = initial_train, 
  assess = initial_test,
  skip = round(increment * nrow(dt_train)),
  cumulative = T
)

folds
```


### Data Preprocessing Recipe

```{r}
xg_rec <- recipe(auction_price_de_ch ~ ., data = dt_train) |>
  update_role(date, new_role = "ID") |>
  step_zv(all_predictors()) |> 
  step_center(all_predictors()) |> 
  step_scale(all_predictors())
```

```{r}
# Applying recipe to training data
xg_rec |> prep() |> juice()

# Applying recipe to testing data
xg_rec |> prep() |> bake(new_data = dt_test)
```

### Initial Test

```{r}
xg_wflow <- workflow() |> 
  add_recipe(xg_rec) |> 
  add_model(
    boost_tree() |> 
      set_engine("xgboost") |> 
      set_mode("regression")
  )
```

```{r}
xg_fit = xg_wflow |> fit(dt_train)
```

```{r}
xg_fit |> 
  augment(dt_test) |> 
  select(date, auction_price_de_ch, .pred) |> 
  rsq(auction_price_de_ch, .pred)
```

```{r}
xg_fit |> 
  augment(dt_test) |> 
  select(date, auction_price_de_ch, .pred) |> 
  rsq(auction_price_de_ch, .pred)
```

```{r}
xg_fit |> 
  augment(dt_test) |> 
  ggplot(aes(auction_price_de_ch, .pred)) +
  geom_point(alpha = 0.2, colour = "midnightblue", size = 2) +
  geom_abline(lty = "dashed", colour = "grey50") +
  scale_x_continuous(labels = scales::comma_format()) +
  scale_y_continuous(labels = scales::comma_format()) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

### Hyperparameter Tuning

Specify the tuning parameters:

```{r}
do_tune = FALSE

if (do_tune){
  xg_spec <- boost_tree(mtry = tune(),
                        trees = tune(),
                        min_n = tune(),
                        tree_depth = tune(),
                        learn_rate = tune(),
                        loss_reduction = tune()) |>
    set_engine("xgboost") |>
    set_mode("regression")
  
  xg_wflow <- workflow() |> 
    add_recipe(xg_rec) |> 
    add_model(xg_spec)
}
```

```{r}
if (do_tune){
  # Setting grid for hyperparameter tuning
  xg_grid <- grid_latin_hypercube(finalize(mtry(), dt_train),
                                  trees(),
                                  min_n(),
                                  tree_depth(),
                                  learn_rate(),
                                  loss_reduction(),
                                  size = 100) # 6.2 hours with 100
  
  start_time = Sys.time()
  # Tuning Hyperparameters
  unregister_dopar <- function() {
    env <- foreach:::.foreachGlobals
    rm(list=ls(name=env), pos=env)
  }
  
  cl <- makePSOCKcluster(6)
  registerDoParallel(cl)
  
  xg_tune <- tune_grid(object = xg_wflow,
                       grid = xg_grid,
                       resamples = folds)
  
  stopCluster(cl)
  unregister_dopar()
  end_time = Sys.time()
  end_time - start_time
  
  # Write results to csv
  xg_tune |> 
    collect_metrics() |> 
    write_csv("1 - XGBoost - Prediction - Tuning Results.csv")
}
```


### Final Fit with best parameters

```{r}
tuning_results = read_csv("1 - XGBoost - Prediction - Tuning Results.csv")

tuning_results |> 
  filter(.metric == "rsq") |> 
  arrange(desc(mean))

tuning_results |> 
  filter(.metric == "rmse") |> 
  arrange(mean)
```


```{r}
best_combination = tuning_results |> 
  filter(.metric == "rsq") |> 
  arrange(desc(mean)) |> 
  head(1)

xg_spec <- boost_tree(
  mtry = best_combination$mtry,
  trees = best_combination$trees,
  min_n = best_combination$min_n,
  tree_depth = best_combination$tree_depth,
  learn_rate = best_combination$learn_rate,
  loss_reduction = best_combination$loss_reduction
) |>
  set_engine("xgboost") |>
  set_mode("regression")

xg_wflow <- workflow() |> 
  add_recipe(xg_rec) |> 
  add_model(xg_spec)
```

Fitting the model:

```{r}
xg_final_fit <- xg_wflow |> 
  fit(dt_train)
```

### Model Evaluation

```{r}
# Get feature importance
xg_final_fit |>
  extract_fit_parsnip() |>
  vip::vi() |> 
  arrange(desc(Importance)) |> 
  head(30) |> 
  ggplot(aes(Importance, fct_reorder(Variable, Importance))) +
  geom_col()
```


```{r}
xg_final_fit |> 
  augment(dt_test) |> 
  ggplot(aes(auction_price_de_ch , .pred)) +
  geom_point(alpha = 0.2, colour = "midnightblue", size = 2) +
  geom_abline(lty = "dashed", colour = "grey50") +
  scale_x_continuous(labels = scales::comma_format()) +
  scale_y_continuous(labels = scales::comma_format()) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

```{r}
xg_final_fit |> 
  augment(dt_test) |> 
  mutate(model = "Gradient Boosting") |> 
  transmute(date, model, prediction = .pred, actual = auction_price_de_ch) |> 
  pivot_longer(-c(date, model)) |> 
  ggplot(aes(date, value, colour = name)) +
  geom_line(alpha = 0.75, linewidth = 0.75) +
  facet_wrap(~ model, nrow = 2) +
  labs(title = "Out-Of-Sample Time Series",
       y = NULL,
       x = NULL,
       colour = NULL) +
  ggsci::scale_colour_jama() +
  scale_y_continuous(labels = scales::comma_format(suffix = " MWh")) +
  theme_bw() +
  theme(plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(face = "italic", size = 12,
                                     colour = "grey50"))
```


