---
title: "0_EDA Tito JAOprice_DE-CH"
author: "Tito Quadri"
date: "2024-04-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(pacman)
pacman::p_load(here, tidyverse)
library(ggsci)
library(scales)

# Default theme for charts
theme_set(
  theme_bw() +
    theme(  
      plot.title = element_text(face = "bold", size = 14),
      plot.subtitle = element_text(
        face = "italic", size = 10, colour = "grey50"
      )
    )
)


```

## Load data
```{r}
df<- read.csv("data_cleaned_UTC_new.csv")

df<-select(df, date, auction_price_de_ch, everything())
names(df)
df$date<- as.Date(df$date)

```


## Drop variables with more than 5% 
```{r}
df %>% is.na() %>% sum()
# 
# columns_with_missing_data <- function(df, threshold = 0.5) {
#   # Calculate percentage of missing values for each column
#   missing_percentages <- colMeans(is.na(df))
#   columns_to_drop <- names(missing_percentages[missing_percentages > threshold])
#   return(columns_to_drop)
# }
# 

# 
# columns_to_drop <- columns_with_missing_data(df, threshold = 0.05)
# df<- df %>% select(-columns_to_drop)
```

## Impute missing values with random forest
```{r}
# df_imputed = missRanger::missRanger(
#   df,
#   num.trees = 3,
#   sample.fraction = 0.1,
#   verbose = 1
# )
# 
# df<- df_imputed
```

## non allocated transmission capacity
```{r}
df |> 
  select(date, auction_price_de_ch, allocatedCapacity_de_ch, ATC_de_ch) |> 
  mutate(difference_allocated = ATC_de_ch - allocatedCapacity_de_ch) |> 
  summary()
```
median is zero and mean is low, often all the capacity is allocated (al almost all) 


## Hitogram target variables
```{r}
df |> 
  select(date, auction_price_de_ch, allocatedCapacity_de_ch, ATC_de_ch) |> 
  pivot_longer(-date) |> 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~ name, scales = "free")
```
auction price follows some generalized pareto distribution, allocated capacity and ATC have a bimodal distribution (hence surely not normal), and are very similar (as suggested before)


## Seasonality

### Yearly

```{r}
df |> 
  mutate(year = year(date))  |> 
  group_by(year) |> 
  summarise(auction_price_de_ch = mean(auction_price_de_ch)) |> 
  ungroup() |> 
  ggplot(aes(year, auction_price_de_ch)) +
  geom_col()
```

There does not seem to be a trend in the average auction price.

### Monthly

```{r}
df |> 
  mutate(month = month(date)) |> 
  ggplot(aes(month, auction_price_de_ch, group = month)) +
  geom_boxplot(outlier.colour = NA) +
  coord_cartesian(ylim = c(0, 80))
```

Auction prices are lower in the summer months, this may suggest that domestic CH is higher in summer (or better, in general consumption is lower)



### Daily

```{r}
df |> 
  mutate(hour = hour(date)) |> 
  ggplot(aes(hour, auction_price_de_ch, group = hour)) +
  geom_boxplot(outlier.colour = NA) +
  coord_cartesian(ylim = c(0, 60))
```

```{r}
df |> 
  mutate(hour = hour(date), quarter = quarter(date)) |> 
  ggplot(aes(hour, auction_price_de_ch, group = hour)) +
  geom_boxplot(outlier.colour = NA) +
  coord_cartesian(ylim = c(0, 70)) +
  facet_wrap(~ quarter, scales = "free")
```

```{r}
df |> 
  mutate(hour = hour(date), month= month(date)) |> 
  ggplot(aes(hour, auction_price_de_ch, group = hour)) +
  geom_boxplot(outlier.colour = NA) +
  coord_cartesian(ylim = c(0, 70)) +
  facet_wrap(~ month, scales = "free")
```

We see that there are big differences between each month hourly distribution, some of them may be grouped (12,1,2 or 5,6,8), some other have specific behaviours (3, 10). One way to group would be by temperature (for example dec, jen, feb; march, november; april, october; ... )




## Correlation

```{r}
# Calculate RANK (SPEARMAN) correlations between predictors and response
correlation <- as.data.frame(cor(df[, -c(1,2)], df[,2], method = "spearman"))
correlation$index=3:93

sorted_corr <- correlation[order(abs(correlation[,"V1"]), decreasing = T),]

corr_predictors <- sorted_corr[1:40,]

corr_names<-colnames(df)[corr_predictors$index]

```

## Random forest importance based on the first 40 corr. var.

```{r}

library(randomForest)

dfone<- df %>% select(c(corr_names,names(df[,c(1,2)])))


# Train a random forest model
rf <- randomForest(auction_price_de_ch ~ ., data = dfone)

# Extract feature importance
feature_importance <- importance(rf, type = 2) %>% as.data.frame()
feature_importance$index=c(sorted_corr$index[1:40],1,2)

View(feature_importance)

sorted_importance <- feature_importance[order(abs(feature_importance$IncNodePurity), decreasing = T),]

var_imp_names<- colnames(df)[sorted_importance$index]

var_imp_names

#RMSE of rf
sqrt(rf$mse[length(rf$mse)])

#SD of auction price in the sample
sd(df$auction_price_de_ch)

```

The RMSE is significantly smaller than the SD of the target wariable, suggesting that the 40 variables taken toghether have predictive power over auction price

Most of them make sense; 

date is one of the most important, suggesting seasonality




## Scatterplots ATC, capacity_forecast_de_lu_ch, allocatedCapacity, crossborder_actual_flow_de_lu_ch

```{r}
plot(df$ATC_de_ch, df$capacity_forecast_de_lu_ch)
```
Clearly highly correlated (as expected), we should use only one of the two in our model


```{r}
plot( df$ATC_de_ch, df$allocatedCapacity_de_ch)
```
Same hold for these two. We shopuld just pick the most precise of the three

```{r}
plot( df$ATC_de_ch, df$crossborder_actual_flow_de_lu_ch)
```
crossborder_actual_flow_de_lu_ch behaves differently and there is not a clear pattern between the two. We can include both in the model


## Lasso
Split train and test
```{r}
library(tidyverse)
library(ggsci)
library(scales)
library(tidymodels)
library(stats)
library(AMR)
set.seed(123)

start_date = min(df$date)
split_date = ymd_hm("2020-01-01 00:00")
end_date = ymd_hm("2021-01-01 00:00")

dt_train = df |> 
  filter(date > start_date) |> 
  filter(date < split_date) |> 
  filter(date < end_date)

dt_test = df |> 
  filter(date > start_date) |> 
  filter(date >= split_date) |> 
  filter(date < end_date)
```

```{r}
# Create recipe
library(recipes)
lasso_rec = recipe(auction_price_de_ch ~ ., data = dt_train) |>
  step_mutate(hour = hour(date)) |> 
  step_date(date) |> 
  update_role(date, new_role = "date") |> 
  step_zv(all_predictors()) |> 
  step_center(all_numeric_predictors()) |> 
  step_scale(all_numeric_predictors()) |> 
  step_dummy(all_nominal_predictors())


summary(lasso_rec) |> as.data.frame()

# Setting up the tuning grid
lasso_grid <- seq(0.00001, 0.2, length.out = 25)

# Tune the parameter
train_metrics = c()
test_metrics = c()

fit_lasso = function(alpha){
  # Setting up specifications
  lasso_spec = linear_reg(mixture = 1, penalty = alpha) %>%
    set_mode("regression") %>%
    set_engine("glmnet")
  
  # Setting up workflow
  lasso_wflow = workflow() |>
    add_model(lasso_spec) |>
    add_recipe(lasso_rec)
  
  # Fit the model with parameter
  lasso_fit = lasso_wflow |> 
    fit(dt_train)
}
library(parsnip)
library(workflows)
library(ModelMetrics)

for (i in 1:length(lasso_grid)){
  
  # Fit lasso with parameter
  lasso_fit = fit_lasso(alpha = lasso_grid[i])
  
  # Make predictions and store the metric
  train_metrics = append(
    train_metrics,
    lasso_fit |> 
      predict(dt_train) |> 
      rmse(dt_train$auction_price_de_ch, .pred) |> 
      pull(.estimate)
  )
  
  test_metrics = append(
    test_metrics,
    lasso_fit |> 
      predict(dt_test) |> 
      rmse(dt_test$auction_price_de_ch, .pred) |> 
      pull(.estimate)
  )
  
  print(paste(i, "done out of", length(lasso_grid)))
}


str(df)

tuning_results = tibble(
  alpha = lasso_grid,
  train = train_metrics,
  test = test_metrics
) 

tuning_results |> 
  pivot_longer(-alpha) |> 
  mutate(value = value^2) |> 
  ggplot(aes(alpha, value, colour = name)) +
  geom_line() +
  geom_point() +
  labs(title = "Train and Test MSE by Penalty")

```


```{r}
# Fitting the model
lasso_fit = fit_lasso(alpha = lasso_grid[5]) #intersect test and train

# Predictions vs actuals in-sample
lasso_fit |> 
  augment(dt_train) |>  
  ggplot(aes(auction_price_de_ch, .pred)) +
  geom_point(alpha = 0.2, colour = "midnightblue", size = 2) +
  geom_abline(lty = "dashed", colour = "grey50") +
  scale_x_continuous(labels = scales::comma_format()) +
  scale_y_continuous(labels = scales::comma_format()) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

# Predictions vs actuals out-of-sample
lasso_fit |> 
  augment(dt_test) |>  
  ggplot(aes(auction_price_de_ch, .pred)) +
  geom_point(alpha = 0.2, colour = "midnightblue", size = 2) +
  geom_abline(lty = "dashed", colour = "grey50") +
  scale_x_continuous(labels = scales::comma_format()) +
  scale_y_continuous(labels = scales::comma_format()) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

# Get the variables that remained in the model
lasso_coef <- lasso_fit |> 
  tidy() |> 
  filter(estimate > 0, 
         ! str_detect(term, "Intercept")) |> 
  pull(term)
```
